{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GGJ9YG3tPwu1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1Yu2VHCr9o",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlfT-jbBzedV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mh63tUL2NJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA = 'europarl'\n",
        "DATA = 'newscarl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adawBJlF6H_K",
        "colab_type": "text"
      },
      "source": [
        "# Downloading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4gJtU8K6ERZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "36d0c517-2273-46df-ca0a-f905a22db6e9"
      },
      "source": [
        "if DATA == 'europarl':\n",
        "  URLS=[\"http://www.statmt.org/europarl/v10/training-monolingual/europarl-v10.es.tsv.gz\"]\n",
        "elif DATA == 'newscarl':\n",
        "    URLS=[\n",
        "    \"http://data.statmt.org/news-crawl/es/news.2007.es.shuffled.deduped.gz\",\n",
        "    \"http://data.statmt.org/news-crawl/es/news.2008.es.shuffled.deduped.gz\",\n",
        "    \"http://data.statmt.org/news-crawl/es/news.2009.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2010.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2011.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2012.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2013.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2014.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2015.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2016.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2017.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2018.es.shuffled.deduped.gz\",\n",
        "    # \"http://data.statmt.org/news-crawl/es/news.2019.es.shuffled.deduped.gz\"\n",
        "  ]\n",
        "\n",
        "for u in URLS:\n",
        "  print(u)\n",
        "  !wget $u\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://data.statmt.org/news-crawl/es/news.2007.es.shuffled.deduped.gz\n",
            "--2020-09-17 15:25:31--  http://data.statmt.org/news-crawl/es/news.2007.es.shuffled.deduped.gz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2705371 (2.6M) [application/x-gzip]\n",
            "Saving to: ‘news.2007.es.shuffled.deduped.gz’\n",
            "\n",
            "news.2007.es.shuffl 100%[===================>]   2.58M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-09-17 15:25:31 (18.0 MB/s) - ‘news.2007.es.shuffled.deduped.gz’ saved [2705371/2705371]\n",
            "\n",
            "http://data.statmt.org/news-crawl/es/news.2008.es.shuffled.deduped.gz\n",
            "--2020-09-17 15:25:31--  http://data.statmt.org/news-crawl/es/news.2008.es.shuffled.deduped.gz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 106965569 (102M) [application/x-gzip]\n",
            "Saving to: ‘news.2008.es.shuffled.deduped.gz’\n",
            "\n",
            "news.2008.es.shuffl 100%[===================>] 102.01M  95.3MB/s    in 1.1s    \n",
            "\n",
            "2020-09-17 15:25:32 (95.3 MB/s) - ‘news.2008.es.shuffled.deduped.gz’ saved [106965569/106965569]\n",
            "\n",
            "http://data.statmt.org/news-crawl/es/news.2009.es.shuffled.deduped.gz\n",
            "--2020-09-17 15:25:32--  http://data.statmt.org/news-crawl/es/news.2009.es.shuffled.deduped.gz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73266366 (70M) [application/x-gzip]\n",
            "Saving to: ‘news.2009.es.shuffled.deduped.gz’\n",
            "\n",
            "news.2009.es.shuffl 100%[===================>]  69.87M  90.5MB/s    in 0.8s    \n",
            "\n",
            "2020-09-17 15:25:33 (90.5 MB/s) - ‘news.2009.es.shuffled.deduped.gz’ saved [73266366/73266366]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKnRHhq17hSh",
        "colab_type": "text"
      },
      "source": [
        "Unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLH9cv7-7ipo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "67fc884f-b12e-40cc-ec88-65047a219af9"
      },
      "source": [
        "if DATA == 'europarl':\n",
        "  FILES = [\"europarl-v10.es.tsv.gz\"]\n",
        "elif DATA == 'newscarl':\n",
        "  FILES=[\n",
        "    \"news.2007.es.shuffled.deduped.gz\",\n",
        "    \"news.2008.es.shuffled.deduped.gz\",\n",
        "    \"news.2009.es.shuffled.deduped.gz\",\n",
        "    # \"news.2010.es.shuffled.deduped.gz\",\n",
        "    # \"news.2011.es.shuffled.deduped.gz\",\n",
        "    # \"news.2012.es.shuffled.deduped.gz\",\n",
        "    # \"news.2013.es.shuffled.deduped.gz\",\n",
        "    # \"news.2014.es.shuffled.deduped.gz\",\n",
        "    # \"news.2015.es.shuffled.deduped.gz\",\n",
        "    # \"news.2016.es.shuffled.deduped.gz\",\n",
        "    # \"news.2017.es.shuffled.deduped.gz\",\n",
        "    # \"news.2018.es.shuffled.deduped.gz\",\n",
        "    # \"news.2019.es.shuffled.deduped.gz\"\n",
        "  ]\n",
        "\n",
        "for f in FILES:\n",
        "  print(f)\n",
        "  !gunzip $f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news.2007.es.shuffled.deduped.gz\n",
            "news.2008.es.shuffled.deduped.gz\n",
            "news.2009.es.shuffled.deduped.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHl_Uh4T6mT1",
        "colab_type": "text"
      },
      "source": [
        "Join all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJKICijI6oZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "aa4ce285-0695-46d1-8c3e-2da16459cc4b"
      },
      "source": [
        "if DATA == 'europarl':\n",
        "  CORPORA = [\"europarl-v10.es.tsv\"]\n",
        "elif DATA == 'newscarl':\n",
        "  CORPORA=[\n",
        "    \"news.2007.es.shuffled.deduped\",\n",
        "    \"news.2008.es.shuffled.deduped\",\n",
        "    \"news.2009.es.shuffled.deduped\",\n",
        "    # \"news.2010.es.shuffled.deduped\",\n",
        "    # \"news.2011.es.shuffled.deduped\",\n",
        "    # \"news.2012.es.shuffled.deduped\",\n",
        "    # \"news.2013.es.shuffled.deduped\",\n",
        "    # \"news.2014.es.shuffled.deduped\",\n",
        "    # \"news.2015.es.shuffled.deduped\",\n",
        "    # \"news.2016.es.shuffled.deduped\",\n",
        "    # \"news.2017.es.shuffled.deduped\",\n",
        "    # \"news.2018.es.shuffled.deduped\",\n",
        "    # \"news.2019.es.shuffled.deduped\"\n",
        "  ]\n",
        "\n",
        "with open('corpus.es', 'w') as outfile:\n",
        "    for fname in CORPORA:\n",
        "        print('Joining file:', fname)\n",
        "        with open(fname) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joining file: news.2007.es.shuffled.deduped\n",
            "Joining file: news.2008.es.shuffled.deduped\n",
            "Joining file: news.2009.es.shuffled.deduped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S884_TeW9fnm",
        "colab_type": "text"
      },
      "source": [
        "Removing extra files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvDgH7sv9hoR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e59fba6-b39d-4a98-bf93-16c0fde53958"
      },
      "source": [
        "!rm *.deduped *.tsv\n",
        "!du -sh corpus.es"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "425M\tcorpus.es\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AE8lAXj_d-w",
        "colab_type": "text"
      },
      "source": [
        "Keep reduced file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhM2nVHz_Sgq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7749b77-9e3e-451e-942a-736d5a5dc0d6"
      },
      "source": [
        "MAX_LINES = 10000\n",
        "\n",
        "with open('corpus.es', 'r') as cfile:\n",
        "  with open('corpus.reduced.es', 'w') as rfile:\n",
        "    count = 0\n",
        "    while count < MAX_LINES:\n",
        "      line = cfile.readline()\n",
        "      rfile.write(line)\n",
        "      count += 1\n",
        "    print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxNJW4UnBQqb",
        "colab_type": "text"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe5MW8QsA_7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE = 'corpus.reduced.es'\n",
        "data = []\n",
        "\n",
        "with open(FILE, 'r') as corpus_file:\n",
        "  Lines = corpus_file.readlines()   \n",
        "  for line in Lines:\n",
        "    data.append(line)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nO-W73VByES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ebe4e4d-5472-4a22-abe5-5edc8e5842f4"
      },
      "source": [
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQBj5qADk5u",
        "colab_type": "text"
      },
      "source": [
        "# Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdMaztZECrWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_words(sentence):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    text = BeautifulSoup(sentence, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"spanish\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7XaPwcEX9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4fc7b24f-bab8-4623-90bc-6ee93ada3e4e"
      },
      "source": [
        "print(data[50])\n",
        "print(sentence_to_words(data[50]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y este final de año ha venido marcado por dos hitos significativos en el campo de los universos paralelos.\n",
            "\n",
            "['final', 'venido', 'marcado', 'do', 'hito', 'significativo', 'campo', 'universo', 'paralelo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNNxwM2FNhrg",
        "colab_type": "text"
      },
      "source": [
        "Also: Remove all sentences with less than 5 words and more than 35"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqESWRpqEjEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "e0562ea1-9b7b-4b69-a03e-2a234bc0987a"
      },
      "source": [
        "import time\n",
        "MAX_PADDING = 35\n",
        "MIN_LEN_SENTENCE = 5\n",
        "preprocessed_data = []\n",
        "start = time.time()\n",
        "perc = 0\n",
        "for ind, s in enumerate(data):\n",
        "  words = sentence_to_words(s)\n",
        "  if len(words) >= MIN_LEN_SENTENCE and len(words) <= MAX_PADDING :\n",
        "    preprocessed_data.append(words)\n",
        "  if ind > perc:\n",
        "    print('{}/{} sentences preprocessed'.format(perc, len(data)))\n",
        "    perc += 1000\n",
        "\n",
        "end = time.time()\n",
        "print(end-start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/10000 sentences preprocessed\n",
            "1000/10000 sentences preprocessed\n",
            "2000/10000 sentences preprocessed\n",
            "3000/10000 sentences preprocessed\n",
            "4000/10000 sentences preprocessed\n",
            "5000/10000 sentences preprocessed\n",
            "6000/10000 sentences preprocessed\n",
            "7000/10000 sentences preprocessed\n",
            "8000/10000 sentences preprocessed\n",
            "9000/10000 sentences preprocessed\n",
            "56.64922022819519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqakgtIfN0Hx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12bb5135-c7bc-470f-cb7f-9d69735effb9"
      },
      "source": [
        "len(preprocessed_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8395"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd7jHHiVE3uR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28f4eeb3-d023-4b43-a9d8-70637650f756"
      },
      "source": [
        "print(preprocessed_data[50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['all', 'quedo', 'pasto', 'buitr', 'perro', 'vagabundo', 'dem', 's', 'animal', 'carro', 'ero']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiR7MsHgFnf1",
        "colab_type": "text"
      },
      "source": [
        "# Build Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAxl-SkE6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "    \n",
        "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
        "    #       sentence is a list of words.\n",
        "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
        "\n",
        "    for sentence in data:\n",
        "        for word in sentence:\n",
        "            if word in word_count:\n",
        "                word_count[word]+=1\n",
        "            else:\n",
        "                word_count[word]=1\n",
        "    \n",
        "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
        "    #       sorted_words[-1] is the least frequently appearing word.\n",
        "    \n",
        "    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "    sorted_words = [tupl[0] for tupl in sorted_words]\n",
        "\n",
        "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 3]): # The -3 is so that we save room for the 'no word'\n",
        "        word_dict[word] = idx + 3                              # 'infrequent' 'pad' labels\n",
        "        \n",
        "    return word_dict, word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7JaLVM1FrnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5b0fd75b-dd14-4169-862e-192a74391915"
      },
      "source": [
        "SIZE_VOCAB = 5000\n",
        "word_dict, complete_dict = build_dict(preprocessed_data, SIZE_VOCAB)\n",
        "list_dict = [key for key in word_dict.keys()]\n",
        "print(list_dict[:20])\n",
        "\n",
        "print('Selected {}/{} words'.format(SIZE_VOCAB, len(complete_dict)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['n', 's', 'm', 'as', 'd', 'est', 'ser', 'espa', 'do', 'tambi', 'si', 'euro', 'l', 'c', 'millon', 'seg', 'ayer', 'p', 'an', 'part']\n",
            "Selected 5000/20827 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyZ0zIx17Vfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will add the XXX as 0 in the dictionary, so when a custom sentence is inputted, the XXX will mark the word the model has to guess\n",
        "word_dict['XXX'] = 0\n",
        "word_dict['PAD'] = 1\n",
        "word_dict['INFREQ'] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D367NLVQHJbI",
        "colab_type": "text"
      },
      "source": [
        "# Convert and pad data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65FbD9edGaKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 1 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 2 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "    \n",
        "    working_sentence = [NOWORD] * pad\n",
        "    \n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "      if word_index < pad:\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "            \n",
        "    return working_sentence, min(len(sentence), pad)\n",
        "\n",
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "    \n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "        \n",
        "    return np.array(result), np.array(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQDj7Uw7HcUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_data, int_data_len = convert_and_pad_data(word_dict, preprocessed_data, MAX_PADDING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDONOpMMHmHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e35e4eea-c55b-430c-cf6e-b8b032f0196b"
      },
      "source": [
        "print(int_data[50], int_data_len[50])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 342    2    2    2    2    2  555    4 1896    2  902    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1] 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFzTv9zKIsow",
        "colab_type": "text"
      },
      "source": [
        "# Extract word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq4cWTxprn10",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad0eb673-abe3-4936-e3f0-6fc16c0ba705"
      },
      "source": [
        "# Check there is no sentence with all 2's\n",
        "int_data_pre = [d for d, lend in zip(int_data, int_data_len) if len(set(d[:lend])) > 1]\n",
        "len_data_pre = [lend for d, lend in zip(int_data, int_data_len) if len(set(d[:lend])) > 1]\n",
        "print('{} of the {} sentences were only 2\\'s'.format(len(int_data)-len(int_data_pre), len(int_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 of the 8395 sentences were only 2's\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQdcCFYI9tY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04f86e05-47f2-4a93-b9bd-11701e1cc722"
      },
      "source": [
        "start = time.time()\n",
        "masked_data = []\n",
        "word_masked = []\n",
        "\n",
        "masked_data = int_data_pre.copy()\n",
        "\n",
        "for idx, (sentence, len_sentence) in enumerate(zip(int_data_pre, len_data_pre)):\n",
        "  acceptable_value = False\n",
        "  \n",
        "  while acceptable_value == False:\n",
        "    idx_word = random.randint(0, len_sentence-1)\n",
        "    if int_data_pre[idx][idx_word] != 2:\n",
        "      acceptable_value = True\n",
        "  \n",
        "  word_masked.append(int_data_pre[idx][idx_word]) #save the word extracted  \n",
        "  masked_data[idx][idx_word] = 0 #put this word to 0\n",
        "\n",
        "end = time.time()\n",
        "print('Run for: {}s'.format(end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run for: 0.03603315353393555s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbfUsgHvLd50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "16b5aee0-908f-4f5d-9380-5fd0e97687d4"
      },
      "source": [
        "print(int_data_pre[50])\n",
        "print(masked_data[50])\n",
        "print(word_masked[50])\n",
        "search2 = [el for el in word_masked if el == 2]\n",
        "print(search2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 342    2    2    2    2    2  555    4 1896    2    0    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1]\n",
            "[ 342    2    2    2    2    2  555    4 1896    2    0    1    1    1\n",
            "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
            "    1    1    1    1    1    1    1]\n",
            "902\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U2lCUE8H3Ze",
        "colab_type": "text"
      },
      "source": [
        "# Split Training, Validation, Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd5FvlfpHoXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, valid_x, train_y, valid_y, train_len, valid_len = train_test_split(masked_data, word_masked, len_data_pre, test_size=0.25, random_state=42)\n",
        "valid_x, test_x, valid_y, test_y, valid_len, test_len = train_test_split(valid_x, valid_y, valid_len, test_size=0.4, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOgfuMCOocL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1dbefa67-12d0-4fc2-f073-7cf85c8f8803"
      },
      "source": [
        "print('train: ', len(train_x), len(train_y), len(train_len))\n",
        "print('valid: ', len(valid_x), len(valid_y), len(valid_len))\n",
        "print('test: ', len(test_x), len(test_y), len(test_len))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:  6294 6294 6294\n",
            "valid:  1259 1259 1259\n",
            "test:  840 840 840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J96V6TBKmd-o",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4AykQrtfcc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_data = None\n",
        "list_dict = None\n",
        "int_data = None\n",
        "int_data_len = None\n",
        "len_data_pre = None\n",
        "int_data_pre = None\n",
        "masked_data = None\n",
        "word_masked = None\n",
        "data = None\n",
        "Lines = None\n",
        "complete_dict = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGJ9YG3tPwu1",
        "colab_type": "text"
      },
      "source": [
        "# TRANSFORMER Model\n",
        "\n",
        "From https://blog.floydhub.com/the-transformer-in-pytorch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYFdLMwxGBt9",
        "colab_type": "text"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EimOEmaDO6AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRMxNt8uGD5I",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i29M7y8GTp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "                \n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:,:seq_len], \\\n",
        "        requires_grad=False).cuda()\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz4vGy41G1hn",
        "colab_type": "text"
      },
      "source": [
        "### Multihead Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFcAsbe3HYsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        \n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "\t# calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onPpr73SHZO0",
        "colab_type": "text"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21XnpcCuHkpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    mask = mask.unsqueeze(1)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "  if dropout is not None:\n",
        "    scores = dropout(scores)\n",
        "  \n",
        "  output = torch.matmul(scores, v)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-JkM6JoHlOR",
        "colab_type": "text"
      },
      "source": [
        "### Final Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo5QyshOHqB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MSlMKo-Hq0B",
        "colab_type": "text"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12jW9X-hHwIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY6PJwRoHxCj",
        "colab_type": "text"
      },
      "source": [
        "## Putting all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjddwjNPHzEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "# build a decoder layer with two multi-head attention layers and\n",
        "# one feed-forward layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model).cuda()\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "          x2 = self.norm_1(x)\n",
        "          x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "          x2 = self.norm_2(x)\n",
        "          x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "          src_mask))\n",
        "          x2 = self.norm_3(x)\n",
        "          x = x + self.dropout_3(self.ff(x2))\n",
        "          return x\n",
        "\n",
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBTRJxJgH_yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDrcTgDOIAU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, size_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(size_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(size_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, size_vocab)\n",
        "\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "\n",
        "        output = self.out(d_output)\n",
        "\n",
        "        return output\n",
        "# we don't perform softmax on the output as this will be handled \n",
        "# automatically by our loss function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqiANlPhGUUL",
        "colab_type": "text"
      },
      "source": [
        "## Input Masks\n",
        "\n",
        "The input mask avoids the possible info by the padding.\n",
        "The target mask is used to avoid looking at future info when guessing next word.\n",
        "\n",
        "It uses the complete sentence without the word as an input for the decoder along with the outputs of the decoder.\n",
        "\n",
        "There are also target masks, when the intention is to not allow the decoder to use future information for the decoding, but in this case we want to use the complete sentence, so there isn't any target mask in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD6q4F5ztwhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mask(data, batchsize):\n",
        "  input_msk = [[[0] if _el == 1 else [1] for _el in _ar] for _ar in data]\n",
        "  arr_input_msk = np.array(input_msk)\n",
        "\n",
        "  torch_msk_input = torch.tensor(input_msk).clone()\n",
        "  mask_sample_ds = torch.utils.data.TensorDataset(torch_msk_input)\n",
        "  msk_input_loader = torch.utils.data.DataLoader(mask_sample_ds, batch_size=batchsize)\n",
        "\n",
        "  return msk_input_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeaEhFrwIESP",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy8WRMft7jsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, mask_loader, eval_loader, eval_mask_loader, epochs, optimizer, loss_fn, device):\n",
        "    print('Start training')\n",
        "    total_length = len(train_loader.dataset)\n",
        "    loss_return = []\n",
        "    eval_loss_return = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        batchs_done = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch, msk_batch in zip(train_loader, mask_loader):\n",
        "            batch_X, batch_y = batch\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            if len(msk_batch)==2:\n",
        "              input_msk, output_msk = msk_batch\n",
        "              input_msk = input_msk.to(device)\n",
        "              output_msk = output_msk.to(device)\n",
        "            else:\n",
        "              input_msk = msk_batch[0]\n",
        "              output_msk = None\n",
        "              input_msk = input_msk.to(device)\n",
        "\n",
        "            out = model(batch_X, batch_X, input_msk, output_msk)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss_fn(out[:, -1, :], batch_y)\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += batch_loss.data.item()\n",
        "\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
        "        loss_return.append(total_loss / len(train_loader))\n",
        "\n",
        "        if epoch % 5 == 0: #every five epochs an evauation\n",
        "          eval_total_loss = 0\n",
        "          model.eval()\n",
        "\n",
        "          for eval_batch, eval_msk_batch in zip(eval_loader, eval_mask_loader):\n",
        "            eval_batch_X, eval_batch_y = eval_batch\n",
        "            eval_batch_X = eval_batch_X.to(device)\n",
        "            eval_batch_y = eval_batch_y.to(device)\n",
        "\n",
        "            if len(eval_msk_batch)==2:\n",
        "              eval_input_msk, eval_output_msk = eval_msk_batch\n",
        "              eval_input_msk = eval_input_msk.to(device)\n",
        "              eval_output_msk = eval_output_msk.to(device)\n",
        "            else:\n",
        "              eval_input_msk = eval_msk_batch[0]\n",
        "              eval_output_msk = None\n",
        "              eval_input_msk = eval_input_msk.to(device)\n",
        "\n",
        "            eval_out = model(eval_batch_X, eval_batch_X, eval_input_msk, eval_output_msk)\n",
        "\n",
        "            eval_batch_loss = loss_fn(eval_out[:, -1, :], eval_batch_y)\n",
        "            eval_total_loss += eval_batch_loss.data.item()\n",
        "\n",
        "          print(\"Eval Loss: {}\".format(eval_total_loss / len(eval_loader)))\n",
        "          eval_loss_return.append(eval_total_loss / len(eval_loader))\n",
        "\n",
        "    return loss_return, eval_loss_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3JoKmYqG9wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check all shapes are of 35 check\n",
        "a = [1 for el in train_x if len(el)!=35]\n",
        "assert len(a) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f70rHuez7nVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "284d4549-6b9f-4e9c-d492-cf312f059054"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "LR = 0.001\n",
        "\n",
        "d_model = 256\n",
        "heads = 8\n",
        "N = 6\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "print('Preparing Input Masks')\n",
        "msk_input_loader = create_mask(train_x, BATCH_SIZE)\n",
        "valid_msk_input_loader = create_mask(valid_x, BATCH_SIZE)\n",
        "\n",
        "print('Preparing Train Data Loaders')\n",
        "train_torch_x = torch.tensor(train_x).clone()\n",
        "train_torch_y = torch.tensor(train_y).clone()\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_torch_x, train_torch_y)\n",
        "train_loader = torch.utils.data.DataLoader(train_sample_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "print('Preparing Validation Data Loaders')\n",
        "valid_torch_x = torch.tensor(valid_x).clone()\n",
        "valid_torch_y = torch.tensor(valid_y).clone()\n",
        "valid_sample_ds = torch.utils.data.TensorDataset(valid_torch_x, valid_torch_y)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_sample_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "print('Initialize Model')\n",
        "model = Transformer(SIZE_VOCAB, d_model, N, heads).to(device)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "print('Define Loss and Optimizer')\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "mask_sample_ds = None\n",
        "train_torch_x = None\n",
        "train_torch_y = None\n",
        "train_sample_ds = None\n",
        "\n",
        "print('Going to train')\n",
        "losses, valid_losses = train(model, train_loader, msk_input_loader, valid_loader, valid_msk_input_loader, EPOCHS, optimizer, loss_function, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Preparing Input Masks\n",
            "Preparing Train Data Loaders\n",
            "Preparing Validation Data Loaders\n",
            "Initialize Model\n",
            "Define Loss and Optimizer\n",
            "Going to train\n",
            "Start training\n",
            "Epoch: 1, Loss: 7.957170886993408\n",
            "Epoch: 2, Loss: 7.0784222030639645\n",
            "Epoch: 3, Loss: 6.996005439758301\n",
            "Epoch: 4, Loss: 6.946495685577393\n",
            "Epoch: 5, Loss: 6.963631687164306\n",
            "Eval Loss: 8.59286584854126\n",
            "Epoch: 6, Loss: 6.980979862213135\n",
            "Epoch: 7, Loss: 6.814035568237305\n",
            "Epoch: 8, Loss: 6.756736421585083\n",
            "Epoch: 9, Loss: 6.660890922546387\n",
            "Epoch: 10, Loss: 6.586739377975464\n",
            "Eval Loss: 9.540335845947265\n",
            "Epoch: 11, Loss: 6.476794557571411\n",
            "Epoch: 12, Loss: 6.420280656814575\n",
            "Epoch: 13, Loss: 6.2891076469421385\n",
            "Epoch: 14, Loss: 6.2116709804534915\n",
            "Epoch: 15, Loss: 6.186410675048828\n",
            "Eval Loss: 10.232352924346923\n",
            "Epoch: 16, Loss: 6.322247467041016\n",
            "Epoch: 17, Loss: 6.16582992553711\n",
            "Epoch: 18, Loss: 5.9779411315917965\n",
            "Epoch: 19, Loss: 6.052376441955566\n",
            "Epoch: 20, Loss: 5.98670298576355\n",
            "Eval Loss: 10.872018718719483\n",
            "Epoch: 21, Loss: 5.760269088745117\n",
            "Epoch: 22, Loss: 5.683951764106751\n",
            "Epoch: 23, Loss: 5.583033304214478\n",
            "Epoch: 24, Loss: 5.386162347793579\n",
            "Epoch: 25, Loss: 5.0755099678039555\n",
            "Eval Loss: 11.165404319763184\n",
            "Epoch: 26, Loss: 4.741726508140564\n",
            "Epoch: 27, Loss: 4.229477732181549\n",
            "Epoch: 28, Loss: 3.75856552362442\n",
            "Epoch: 29, Loss: 3.2437615323066713\n",
            "Epoch: 30, Loss: 2.7630026972293855\n",
            "Eval Loss: 11.634246730804444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbH8W3Ex9Tvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2f94468b-6fa0-4454-fe4a-e1757309ae60"
      },
      "source": [
        "# We can make a plot to observe better\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses, 'b')\n",
        "x_axes_val = [i*5+5 for i in range(len(valid_losses))]\n",
        "plt.plot(x_axes_val, valid_losses, '--r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brfvvdxyIRPx",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSoFhQ2is-rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_loader, masks_loader, loss_fn, batch_size, device):  \n",
        "  print('Start evaluating')\n",
        "  total_loss = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch, msk_batch in zip(test_loader, masks_loader):\n",
        "        batch_X, batch_y = batch\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        if len(msk_batch)==2:\n",
        "          input_msk, output_msk = msk_batch\n",
        "          input_msk = input_msk.to(device)\n",
        "          output_msk = output_msk.to(device)\n",
        "        else:\n",
        "          input_msk = msk_batch[0]\n",
        "          output_msk = None\n",
        "          input_msk = input_msk.to(device)\n",
        "\n",
        "        len_batch = len(batch_X)\n",
        "\n",
        "        out = model(batch_X, batch_X, input_msk, output_msk)\n",
        "\n",
        "        batch_loss = loss_fn(out[:, -1, :], batch_y)\n",
        "        total_loss += batch_loss.data.item()\n",
        "\n",
        "  return total_loss / len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGW5M3MBtMiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d5493899-7bc6-401e-b9ae-2791695f6d84"
      },
      "source": [
        "print('Preparing Input Masks')\n",
        "msk_test_input_loader = create_mask(test_x, BATCH_SIZE)\n",
        "\n",
        "print('Preparing Data Loaders')\n",
        "test_torch_x = torch.tensor(test_x).clone()\n",
        "test_torch_y = torch.tensor(test_y).clone()\n",
        "test_sample_ds = torch.utils.data.TensorDataset(test_torch_x, test_torch_y)\n",
        "test_loader = torch.utils.data.DataLoader(test_sample_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_torch_x = None\n",
        "test_torch_y = None\n",
        "test_sample_ds = None\n",
        "\n",
        "print('Going to test')\n",
        "test_loss = evaluate(model, test_loader, msk_test_input_loader, loss_function, BATCH_SIZE, device)\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing Input Masks\n",
            "Preparing Data Loaders\n",
            "Going to test\n",
            "Start evaluating\n",
            "1.6193408584594726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xdzH5JJ4pRx",
        "colab_type": "text"
      },
      "source": [
        "# Custom Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU_sP6OxbP9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def guess_word(custom_sentence):\n",
        "  # PREPROCESS\n",
        "  custom_tok = sentence_to_words(custom_sentence)\n",
        "\n",
        "  # LIMIT LENGTH\n",
        "  if len(custom_tok) > MAX_PADDING:\n",
        "    custom_tok = custom_tok[:MAX_PADDING]\n",
        "\n",
        "  # CONVERT AND PAD\n",
        "  custom_int, custom_len = convert_and_pad(word_dict, custom_tok, MAX_PADDING)\n",
        "\n",
        "  # MASK\n",
        "  msk_input_custom = [[0] if _el == 1 else [1] for _el in custom_int]\n",
        "  torch_msk_input = torch.tensor(msk_input_custom).clone().unsqueeze(0).to(device)\n",
        "  output_mask = None\n",
        "\n",
        "\n",
        "  #PASS THROUGH THE MODEL\n",
        "  torch_custom = torch.tensor(custom_int).clone().unsqueeze(0).to(device)\n",
        "  model.eval()\n",
        "  custom_out = model(torch_custom, torch_custom, torch_msk_input, output_mask)\n",
        "\n",
        "  custom_out = custom_out[:,-1,:].cpu().detach().numpy().reshape(-1)\n",
        "  ind_max = np.argmax(custom_out, axis=0)\n",
        "\n",
        "  for key, value in word_dict.items(): \n",
        "    if ind_max == value:\n",
        "      resulting_word = key\n",
        "      \n",
        "  return resulting_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68FI4Ytm50F5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "992c81f8-1059-4210-e8c7-2b773caa8bfa"
      },
      "source": [
        "test_sentences = [\n",
        "  \"Ha habido una XXX en Colombia durant la presentacion del presidente\",\n",
        "  \"Todas las tropas han sido XXX a America\",\n",
        "  \"Estaba pensando que quizas XXX deberias hacerlo\",\n",
        "  \"Todo lo que llevo esta dentro de mi XXX\"\n",
        "]\n",
        "\n",
        "for custom_sentence in test_sentences:\n",
        "  resulting_word = guess_word(custom_sentence)\n",
        "\n",
        "  print('Initial Sentence: \\t {}'.format(custom_sentence))\n",
        "  print('Word Guessed: \\t\\t {}'.format(resulting_word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Sentence: \t Ha habido una XXX en Colombia durant la presentacion del presidente\n",
            "Word Guessed: \t\t hab\n",
            "Initial Sentence: \t Todas las tropas han sido XXX a America\n",
            "Word Guessed: \t\t hab\n",
            "Initial Sentence: \t Estaba pensando que quizas XXX deberias hacerlo\n",
            "Word Guessed: \t\t s\n",
            "Initial Sentence: \t Todo lo que llevo esta dentro de mi XXX\n",
            "Word Guessed: \t\t n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ln5p_iLIGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}